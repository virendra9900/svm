{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assisgnment code - DA-AG-013"
      ],
      "metadata": {
        "id": "ocesYAdZQeu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm mainly used for classification tasks, but it can also be used for regression. It is especially powerful for binary classification problems, where the goal is to separate data points into two categories.\n",
        "\n",
        "**How SVM Works:**  \n",
        "SVM works by finding the best decision boundary (called a hyperplane) that separates the data into different classes. This hyperplane is chosen in such a way that the margin between the classes is as large as possible. The margin is defined as the distance between the hyperplane and the closest data points from each class. These closest points are called support vectors, and they play a key role in defining the position and orientation of the hyperplane.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "- **Hyperplane:**  \n",
        "  A line (in 2D), a plane (in 3D), or a higher-dimensional surface that divides the dataset into different classes.\n",
        "\n",
        "- **Margin:**  \n",
        "  The distance between the hyperplane and the nearest data points from each class. A larger margin usually leads to better generalization.\n",
        "\n",
        "- **Support Vectors:**  \n",
        "  These are the data points closest to the hyperplane. They are \"supporting\" the hyperplane and directly affect its position.\n",
        "\n",
        "- **Linear vs. Non-linear Separation:**  \n",
        "  - If data is linearly separable, SVM finds a straight line (or plane) to divide the classes.  \n",
        "  - If data is not linearly separable, SVM uses a technique called the **kernel trick** to project the data into a higher dimension where it becomes separable.\n",
        "\n",
        "- **Kernel Trick:**  \n",
        "  A mathematical function used to transform the data into a higher-dimensional space. This makes it possible to separate non-linear data using a linear boundary in that space.\n",
        "\n",
        "- **Regularization (Soft Margin):**  \n",
        "  SVM can allow some misclassifications to prevent overfitting. This is called using a soft margin, which gives the model flexibility when dealing with noisy or overlapping data.\n",
        "\n",
        "**Advantages of SVM:**\n",
        "- Works well for high-dimensional data.\n",
        "- Effective even when the number of features is greater than the number of samples.\n",
        "- Can handle non-linear data using kernels.\n",
        "\n",
        "**Disadvantages of SVM:**\n",
        "- Training can be slow for very large datasets.\n",
        "- Performance depends heavily on the choice of kernel and its parameters.\n",
        "- Not suitable for datasets with a lot of noise and overlapping classes.\n",
        "\n",
        "**Real-life Example:**  \n",
        "Suppose we are building an email spam detector. An SVM can learn to classify emails as spam or not spam by analyzing the words in the emails and learning the optimal decision boundary that separates spam from legitimate messages.\n",
        "\"\"\"\n",
        "cells.append(nbf.new_markdown_cell(q1))"
      ],
      "metadata": {
        "id": "z65K8x6_Qy2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "In Support Vector Machine (SVM), the goal is to find the best boundary (called a hyperplane) that separates the data into classes. Depending on how strictly the model separates the classes, there are two types of margins: Hard Margin and Soft Margin.\n",
        "\n",
        "#### 1. Hard Margin SVM\n",
        "A Hard Margin SVM is the strictest version of SVM. It tries to separate the data with no errors at all.\n",
        "\n",
        "**Characteristics:**\n",
        "- Assumes the data is perfectly linearly separable.\n",
        "- No data points are allowed to lie within the margin or on the wrong side of the boundary.\n",
        "- Creates the maximum margin between classes without any tolerance.\n",
        "\n",
        "**Advantages:**\n",
        "- Simple and works well for clean, noise-free data.\n",
        "- Perfect classification on training data.\n",
        "\n",
        "**Disadvantages:**\n",
        "- Very sensitive to outliers.\n",
        "- Fails when the data is not perfectly separable or contains noise.\n",
        "- Can lead to overfitting in practical datasets.\n",
        "\n",
        "#### 2. Soft Margin SVM\n",
        "A Soft Margin SVM allows some misclassifications to occur.\n",
        "\n",
        "**Characteristics:**\n",
        "- Accepts that real-world data may be noisy or overlapping.\n",
        "- Introduces flexibility by allowing some points to be within the margin or misclassified.\n",
        "- Controlled by parameter **C**:\n",
        "  - A large C = strict separation, less tolerance to errors.\n",
        "  - A small C = more tolerance, allows a wider margin.\n",
        "\n",
        "**Advantages:**\n",
        "- Works better on real-world datasets with noise and overlapping classes.\n",
        "- Helps prevent overfitting.\n",
        "- More robust than hard margin SVM.\n",
        "\n",
        "**Disadvantages:**\n",
        "- Requires careful tuning of the C parameter.\n",
        "- Slightly more complex to implement and understand.\n",
        "\n",
        "**Summary Table:**\n",
        "\n",
        "| Feature              | Hard Margin SVM        | Soft Margin SVM          |\n",
        "|----------------------|------------------------|---------------------------|\n",
        "| Tolerance to errors  | No (strict separation) | Yes (allows errors)       |\n",
        "| Handles noisy data   | Poorly                 | Better                    |\n",
        "| Real-world use       | Limited                | Preferred                 |\n",
        "| Controlled by        | No                     | Yes, parameter C          |\n",
        "| Overfitting risk     | High                   | Lower                     |\n",
        "\n",
        "**Example:**  \n",
        "Classifying emails as spam: hard margin assumes perfect separation, while soft margin allows some flexibility for better generalization.\n",
        "\"\"\"\n",
        "cells.append(nbf.new_markdown_cell(q2))"
      ],
      "metadata": {
        "id": "Y_QDMdmnQ6BT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
        "\n",
        "In Support Vector Machine (SVM), we try to find a hyperplane that separates the classes of data. However, in many real-world problems, the data is not linearly separable.\n",
        "\n",
        "#### What is the Kernel Trick?\n",
        "The Kernel Trick is a mathematical technique used in SVM to transform non-linearly separable data into a higher-dimensional space where it becomes linearly separable.\n",
        "\n",
        "**Advantages:**\n",
        "- Allows SVM to solve non-linear problems.\n",
        "- Saves computation by not transforming the data explicitly.\n",
        "- Suitable for complex data like images, texts, or biological sequences.\n",
        "\n",
        "#### Common Kernel Functions:\n",
        "- **Linear Kernel:** K(x, y) = x ‚Ä¢ y  \n",
        "- **Polynomial Kernel:** K(x, y) = (x ‚Ä¢ y + c)^d  \n",
        "- **Radial Basis Function (RBF) / Gaussian Kernel:** K(x, y) = exp(-Œ≥ ||x ‚àí y||¬≤)  \n",
        "- **Sigmoid Kernel:** K(x, y) = tanh(Œ± x ‚Ä¢ y + c)\n",
        "\n",
        "#### Example: RBF Kernel\n",
        "Suppose we are classifying data in concentric circles. In 2D, these cannot be separated linearly. The RBF kernel transforms the data into a higher dimension where a straight line (hyperplane) can separate the classes.\n",
        "\n",
        "#### When to Use Which Kernel?\n",
        "\n",
        "| Kernel     | Use Case                                 |\n",
        "|------------|-------------------------------------------|\n",
        "| Linear     | Linearly separable data                  |\n",
        "| Polynomial | Medium-complexity data with interactions |\n",
        "| RBF        | Non-linear data with complex boundaries  |\n",
        "| Sigmoid    | Rarely used, inspired by neural nets     |\n",
        "\n",
        "**Real-Life Example:**  \n",
        "In handwriting recognition, the RBF kernel can transform pixel data so SVM can classify digits effectively.\n",
        "\"\"\"\n",
        "cells.append(nbf.new_markdown_cell(q3))\n"
      ],
      "metadata": {
        "id": "19tecSI9RCQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Question 4: What is a Na√Øve Bayes Classifier, and why is it called ‚Äúna√Øve‚Äù?\n",
        "\n",
        "A Na√Øve Bayes Classifier is a simple and efficient supervised learning algorithm based on **Bayes‚Äô Theorem**, mainly used for classification.\n",
        "\n",
        "#### Bayes‚Äô Theorem:\n",
        "P(Class | Data) = [P(Data | Class) √ó P(Class)] / P(Data)\n",
        "\n",
        "#### Why is it called ‚ÄúNa√Øve‚Äù?\n",
        "Because it assumes **feature independence** ‚Äî each feature contributes independently to the final probability. This assumption is rarely true but simplifies computation and works surprisingly well.\n",
        "\n",
        "#### How it works:\n",
        "1. Calculates probability of data given each class (likelihood).\n",
        "2. Multiplies by prior probability of each class.\n",
        "3. Chooses class with highest posterior probability.\n",
        "\n",
        "#### Example:\n",
        "Classifying an email as spam using word presence. Assumes each word‚Äôs appearance is independent of the others.\n",
        "\n",
        "#### Advantages:\n",
        "- Fast, simple, and scalable.\n",
        "- Works well with high-dimensional data (like text).\n",
        "- Requires small training data.\n",
        "\n",
        "#### Disadvantages:\n",
        "- Assumes feature independence.\n",
        "- Struggles with correlated features.\n",
        "- Cannot model complex relationships.\n",
        "\n",
        "#### Real-Life Applications:\n",
        "- Spam filtering\n",
        "- Sentiment analysis\n",
        "- Disease prediction\n",
        "- Document categorization\n",
        "\"\"\"\n",
        "cells.append(nbf.new_markdown_cell(q4))\n"
      ],
      "metadata": {
        "id": "b5mit0XARf2o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Question 5: Describe the Gaussian, Multinomial, and Bernoulli Na√Øve Bayes variants. When would you use each one?\n",
        "\n",
        "#### 1. Gaussian Na√Øve Bayes\n",
        "- Used for continuous numeric features.\n",
        "- Assumes normal distribution of features.\n",
        "- Example: temperature, age, sensor data.\n",
        "\n",
        "#### 2. Multinomial Na√Øve Bayes\n",
        "- Used for count-based data (e.g., word frequency).\n",
        "- Inputs must be non-negative integers.\n",
        "- Example: spam detection using word counts, document classification.\n",
        "\n",
        "#### 3. Bernoulli Na√Øve Bayes\n",
        "- Used for binary features (presence/absence).\n",
        "- Ignores frequency; only cares about existence of features.\n",
        "- Example: if a word appears in an email (1) or not (0).\n",
        "\n",
        "#### Summary Table:\n",
        "\n",
        "| Variant              | Data Type      | Feature Type      | Use Case                        |\n",
        "|----------------------|----------------|--------------------|----------------------------------|\n",
        "| Gaussian             | Continuous      | Real-valued        | Medical/sensor data             |\n",
        "| Multinomial          | Discrete counts | Word frequency     | Text classification, spam       |\n",
        "| Bernoulli            | Binary          | 0/1 (presence)     | Email filtering, short texts    |\n",
        "\n",
        "#### Important Note:\n",
        "- Use **Gaussian** for real numbers.\n",
        "- Use **Multinomial** for count data.\n",
        "- Use **Bernoulli** for binary features.\n",
        "\"\"\"\n",
        "cells.append(nbf.new_markdown_cell(q5))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XtvSYWt8up_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Info:\n",
        "‚óè You can use any suitable datasets like Iris, Breast Cancer, or Wine from\n",
        "sklearn.datasets or a CSV file you have.\n",
        "\n",
        "\n",
        "Question 6: Write a Python program to:\n",
        "‚óè Load the Iris dataset\n",
        "‚óè Train an SVM Classifier with a linear kernel\n",
        "‚óè Print the model's accuracy and support vectors.\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "9PtKL_s6OVfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: Train an SVM Classifier on the Iris Dataset**\n",
        "\n",
        "We will use the `Iris` dataset from `sklearn.datasets` and train an **SVM (Support Vector Machine)** classifier using a **linear kernel**.\n",
        "\n",
        "Steps:\n",
        "1. Load the Iris dataset\n",
        "2. Split the data into training and testing sets\n",
        "3. Train an SVM classifier\n",
        "4. Print:\n",
        "   - Accuracy of the model on the test set\n",
        "   - Support vectors used by the model\n"
      ],
      "metadata": {
        "id": "_AMTf8tluRPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create SVM model with linear kernel\n",
        "model = SVC(kernel='linear')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy and support vectors\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"\\nNumber of Support Vectors for Each Class:\", model.n_support_)\n",
        "print(\"\\nSupport Vectors:\")\n",
        "print(model.support_vectors_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXg6EjrouaCA",
        "outputId": "20fbf1b4-d2fd-4ba7-87f7-6109618a2719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "\n",
            "Number of Support Vectors for Each Class: [ 3 11 11]\n",
            "\n",
            "Support Vectors:\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "‚óè Load the Breast Cancer dataset\n",
        "‚óè Train a Gaussian Na√Øve Bayes model\n",
        "‚óè Print its classification report including precision, recall, and F1-score.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "YN-Zn_5OujYv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Train a Gaussian Na√Øve Bayes Model on the Breast Cancer Dataset**\n",
        "\n",
        "We will use the `Breast Cancer` dataset from `sklearn.datasets` and perform classification using the **Gaussian Na√Øve Bayes** algorithm.\n",
        "\n",
        "Steps:\n",
        "1. Load the dataset\n",
        "2. Split it into training and testing sets\n",
        "3. Train a Gaussian Na√Øve Bayes classifier\n",
        "4. Print the classification report (precision, recall, F1-score)\n"
      ],
      "metadata": {
        "id": "ubLj5tBhu_LP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Gaussian Naive Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoVtZIG1vByM",
        "outputId": "519c3989-e0b8-42f4-99ca-809695fdb377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "‚óè Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "‚óè Print the best hyperparameters and accuracy.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "Lk84F-AevKHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: SVM Classifier on Wine Dataset with Hyperparameter Tuning (GridSearchCV)**\n",
        "\n",
        "We will:\n",
        "1. Load the Wine dataset from sklearn\n",
        "2. Train an SVM classifier\n",
        "3. Use `GridSearchCV` to find the best `C` and `gamma`\n",
        "4. Print the best parameters and accuracy\n"
      ],
      "metadata": {
        "id": "MAPgkrfxvMYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf']  # using RBF kernel\n",
        "}\n",
        "\n",
        "# Create the SVM model and apply GridSearchCV\n",
        "svm = SVC()\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model and evaluate\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Output results\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Test Set Accuracy:\", round(accuracy * 100, 2), \"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YACwdcDxvOfn",
        "outputId": "b27e6201-dca1-4b73-d7e0-849f0ba32dc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Test Set Accuracy: 83.33 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "‚óè Train a Na√Øve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "‚óè Print the model's ROC-AUC score for its predictions.\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "OOn8pm2Evck8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Na√Øve Bayes on Text Data with ROC-AUC Score**\n",
        "\n",
        "We will:\n",
        "1. Load a subset of the `20newsgroups` dataset using `sklearn.datasets`\n",
        "2. Preprocess the text using `TfidfVectorizer`\n",
        "3. Train a Multinomial Na√Øve Bayes classifier\n",
        "4. Use `roc_auc_score` to evaluate the model\n",
        "\n",
        "Note: ROC-AUC score is used for binary classification, so we'll use only two categories.\n"
      ],
      "metadata": {
        "id": "HgTvRzkwveS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Load 2 classes for binary classification (e.g., 'sci.space' and 'rec.autos')\n",
        "categories = ['sci.space', 'rec.autos']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(newsgroups.data)\n",
        "y = newsgroups.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Na√Øve Bayes Classifier\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for ROC-AUC\n",
        "y_prob = nb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC-AUC Score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(\"ROC-AUC Score:\", round(roc_auc, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_u0vDpzvgpv",
        "outputId": "87442d38-38ad-4988-ba10-ead42ec8e985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you‚Äôre working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "‚óè Text with diverse vocabulary\n",
        "‚óè Potential class imbalance (far more legitimate emails than spam)\n",
        "‚óè Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "‚óè Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "‚óè Choose and justify an appropriate model (SVM vs. Na√Øve Bayes)\n",
        "‚óè Address class imbalance\n",
        "‚óè Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "ATC9kdgSvou6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Spam Email Classification ‚Äî Full Pipeline**\n",
        "\n",
        "We are tasked to classify emails as Spam or Not Spam.\n",
        "\n",
        "### üßπ 1. Data Preprocessing:\n",
        "- **Handle Missing Values:** Replace missing email bodies with empty strings.\n",
        "- **Text Vectorization:** Use `TfidfVectorizer` to convert text into numerical features.\n",
        "- **Train-test split:** Reserve part of the data for testing.\n",
        "\n",
        "### ü§ñ 2. Model Selection:\n",
        "- **Na√Øve Bayes (MultinomialNB)** is ideal for text data ‚Äî fast and efficient.\n",
        "- **SVM** is powerful but computationally heavier. Not ideal for high-dimensional sparse text unless tuned well.\n",
        "- We‚Äôll use **MultinomialNB** for a practical, real-world solution.\n",
        "\n",
        "### ‚öñÔ∏è 3. Handle Class Imbalance:\n",
        "- Use **class weights**, **oversampling (e.g., SMOTE)**, or **stratified sampling**.\n",
        "- We‚Äôll simulate imbalance and then use `class_weight='balanced'` or `resample`.\n",
        "\n",
        "### üìä 4. Evaluation Metrics:\n",
        "- **Precision**: Important to reduce false positives (marking legit email as spam).\n",
        "- **Recall**: Important to catch as much spam as possible.\n",
        "- **F1-Score**: Harmonic balance between precision and recall.\n",
        "- **ROC-AUC**: Overall model discrimination ability.\n",
        "\n",
        "### üíº 5. Business Impact:\n",
        "- Preventing spam saves time and reduces risk.\n",
        "- Avoiding false positives ensures important emails aren‚Äôt lost.\n",
        "- Balanced filtering improves user trust in the email platform.\n"
      ],
      "metadata": {
        "id": "JgdHfYCkvvtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# üî∏ Simulate Dataset (You can replace this with a real CSV file)\n",
        "data = {\n",
        "    'email_text': [\n",
        "        \"Congratulations! You've won a lottery, claim now!\",\n",
        "        \"Dear user, your invoice is attached\",\n",
        "        \"Win big prizes!!! Click here now\",\n",
        "        np.nan,\n",
        "        \"Meeting at 3pm with the sales team.\",\n",
        "        \"Limited time offer, buy now!\",\n",
        "        \"Reminder: Project deadline tomorrow\",\n",
        "        \"Exclusive deal just for you, buy today!\",\n",
        "        \"Lunch with client at noon\",\n",
        "        \"You are selected for a prize. Send details!\"\n",
        "    ],\n",
        "    'label': [1, 0, 1, 0, 0, 1, 0, 1, 0, 1]  # 1 = Spam, 0 = Not Spam\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# üîπ Step 1: Handle Missing Data\n",
        "df['email_text'] = df['email_text'].fillna(\"\")\n",
        "\n",
        "# üîπ Step 2: Simulate Class Imbalance (Optional)\n",
        "df_majority = df[df.label == 0]\n",
        "df_minority = df[df.label == 1]\n",
        "df_majority_downsampled = resample(df_majority, replace=False, n_samples=len(df_minority), random_state=42)\n",
        "df_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
        "\n",
        "# üîπ Step 3: Vectorize Text\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df_balanced['email_text'])\n",
        "y = df_balanced['label']\n",
        "\n",
        "# üîπ Step 4: Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42, test_size=0.3)\n",
        "\n",
        "# üîπ Step 5: Train Model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# üîπ Step 6: Evaluate Model\n",
        "y_pred = model.predict(X_test)\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"üîπ Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nüîπ Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"\\nüîπ ROC-AUC Score:\", round(roc_auc_score(y_test, y_prob), 4))\n"
      ],
      "metadata": {
        "id": "yd9gkd8jvwZQ",
        "outputId": "c4b83f68-b7ef-483c-b558-3cd5a5a8588e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         2\n",
            "           1       0.33      1.00      0.50         1\n",
            "\n",
            "    accuracy                           0.33         3\n",
            "   macro avg       0.17      0.50      0.25         3\n",
            "weighted avg       0.11      0.33      0.17         3\n",
            "\n",
            "\n",
            "üîπ Confusion Matrix:\n",
            "[[0 2]\n",
            " [0 1]]\n",
            "\n",
            "üîπ ROC-AUC Score: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}